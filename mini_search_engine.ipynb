{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3e90da-f824-4d51-a826-d8d8e3dbcef2",
   "metadata": {},
   "source": [
    "# Building a Mini Search Engine: From Retrieval to Evaluation\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96148c-4cfb-482c-b9d0-9144676e4555",
   "metadata": {},
   "source": [
    "In this notebook, we will build a miniature text retrieval system using the classic Cranfield collection. We will walk through the entire process, from loading and preprocessing data to evaluating our search engine's performance. Our dataset is a pre-processed version of the \"Cranfield collections,\" which contains 1,000 short documents from the field of aerodynamics, along with 225 queries and their corresponding relevance judgments (i.e., which documents are considered relevant to which queries by human experts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc717cf9-42f8-4d78-af00-e70816336159",
   "metadata": {},
   "source": [
    "#### All files are available in the linked git repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8382edf-66d2-4af8-9529-f8ed2b472761",
   "metadata": {},
   "source": [
    "## 1. Setting Up\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542b0cab-2c33-4f46-8f3a-c425ef7307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import xml.etree.ElementTree as ET\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2815ee36-215f-40c4-98ee-904d0a165c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to assets/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to assets/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_path = \"assets/nltk_data\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# Download stopwords to the specified path\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "# Add the path to NLTK's search path\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412ffdd-3392-4e7e-97ee-b38dcf1331ab",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing Data\n",
    "\n",
    "### 2.1 Loading Documents\n",
    "\n",
    "Let's create a function to load and preprocess our documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b49491-d169-413d-8e66-890ae4fe202a",
   "metadata": {},
   "source": [
    "The function below loads all Cranfield documents. Each of the 1,000 documents is stored as a single text file under `assets/cranfield/cranfieldDocs`. While standard file-reading procedures can work here, you will notice that the content format closely resembles that of an HTML/XML file, with multiple \"fields\" marked by pairs of tags like `<TEXT>` and `</TEXT>`. This allows us to use XML-parsing tools in Python, such as the built-in [`xml.etree.ElementTree`](https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree) or external packages like [`lxml`](https://lxml.de/), which may be more efficient than reading the files line by line.\n",
    "\n",
    "We consider anything between `<TEXT>` and `</TEXT>` as the \"content\" of a document. Additionally, we need the identifier between `<DOCNO>` and `</DOCNO>` to uniquely identify each document.\n",
    "\n",
    "The steps followed are:\n",
    "\n",
    "* **Tokenize the text.** We use `word_tokenize` from the `nltk` library for quick tokenization.\n",
    "\n",
    "* **Remove stop words.** Words like \"a\", \"for\", \"the\", etc., appear in almost every document and do not help distinguish one document from another. We remove them to drastically reduce the size of the vocabulary we have to deal with. The `stopwords` utility from `nltk.corpus` provides us with a list of stop words in English.\n",
    "\n",
    "* **Stem each word.** Words sharing a common stem usually have related meanings. For example, \"compute\", \"computer\", and \"computation\" all share the same stem. We use `PorterStemmer` from `nltk.stem.porter` to stem each word, further shrinking our vocabulary by keeping only the word stems.\n",
    "\n",
    "Now that each document is comprised of a sequence of tokens, we store all documents in a `dict` indexed by the `doc_id`s, similar to the following:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'd1'   : ['experiment', 'studi', ..., 'configur', 'experi', '.'], \n",
    "    'd2'   : ['studi', 'high-spe', ..., 'steadi', 'flow', '.'],\n",
    "    ..., \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40ea252-82e4-49e8-a668-67da3d69afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cranfield_docs():\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Load the list of English stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store documents\n",
    "    doc = {}\n",
    "    # Traverse the directory containing Cranfield documents\n",
    "    for root_folder, _, files in os.walk('assets/cranfield/cranfieldDocs'):\n",
    "        for filename in files:\n",
    "            # Check if the file is a Cranfield document based on filename format\n",
    "            if filename[-4:].isdigit():\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(root_folder, filename)\n",
    "                # Parse the XML file\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                # Tokenize the text within <TEXT> tags\n",
    "                toks = word_tokenize(root.find('TEXT').text.strip())\n",
    "                # Create a list of stemmed tokens, excluding stop words\n",
    "                doc['d' + root.find('DOCNO').text.strip()] = [stemmer.stem(tok) for tok in toks if tok not in stop_words]\n",
    "    # Return the dictionary of documents, sorted by document ID\n",
    "    return dict(sorted(doc.items(), key=lambda item: int(item[0][1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8808bb4-9fdc-48bd-9255-2f79ee4a30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents = load_cranfield_docs()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a58f44-bb1f-45c9-9eab-5995425ede88",
   "metadata": {},
   "source": [
    "### 2.2 Loading Queries\n",
    "\n",
    "Now, let's load and preprocess our queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296bd88-136f-4b4f-98d9-c5c76af05c54",
   "metadata": {},
   "source": [
    "The function below loads the queries. The queries are all stored in a single text file, `assets/cranfield/cranfield.queries`.\n",
    "\n",
    "We will identify each query similarly to how we identify each document. Each query is assigned a `q_id` formed by prefixing the query number at the beginning of each line with the letter \"q\". We will also need to perform the same tokenization, stop words removal, and word stemming on each piece of query text as we have done on the documents. Additionally, we will drop the ending period (\".\") that is a stand-alone token at the end of every query.\n",
    "\n",
    "The processed queries will be stored in a dictionary, like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1'  : ['similar', 'law', ..., 'speed', 'aircraft'],\n",
    "    'q2'  : ['structur', 'aeroelast', ..., 'speed', 'aircraft'], \n",
    "    ...\n",
    "    'q225': ['design', 'factor', ..., 'number', '5']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7754055-3e74-4887-a29c-27ddc37b42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cranfield_queries():\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Load the list of English stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Initialize an empty dictionary to store queries\n",
    "    queries = {}\n",
    "    # Open the file containing Cranfield queries\n",
    "    with open('assets/cranfield/cranfield.queries', 'r') as file:\n",
    "        for line in file:\n",
    "            # Tokenize the query line\n",
    "            toks = word_tokenize(line)\n",
    "            # Create a list of stemmed tokens, excluding stop words\n",
    "            cont = [stemmer.stem(tok) for tok in toks if tok not in stop_words]\n",
    "            # Store the processed query in the dictionary with a query ID\n",
    "            queries['q' + cont[0]] = cont[1:]\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f8e26f-7a5f-4092-88b3-4d8da12a3a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 225 queries\n"
     ]
    }
   ],
   "source": [
    "# Load the queries\n",
    "queries = load_cranfield_queries()\n",
    "print(f\"Loaded {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d45652-3c66-4bbe-9be4-446c539fe3a4",
   "metadata": {},
   "source": [
    "### 2.3 Loading Relevance Judgments\n",
    "\n",
    "Finally, let's load the relevance judgments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccdedd-cfcc-424b-aa6a-efc7250c37e7",
   "metadata": {},
   "source": [
    "\n",
    "In our notation, this can be interpreted as:\n",
    "\n",
    "- `d184` is relevant to `q1`\n",
    "- `d29` is relevant to `q1`\n",
    "- `d31` is relevant to `q1`\n",
    "\n",
    "A query could have any number of relevant documents (or none). We store all the relevance judgments in a dictionary indexed by `q_ids`, similar to:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1': ['d184', 'd29', ..., 'd879', 'd880'],\n",
    "    'q2': ['d12', 'd15', ..., 'd864', 'd658'], \n",
    "    ...\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbf2820-ef4b-4ebc-aee3-6154db0c0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cranfield_reljudges():\n",
    "    # Initialize an empty dictionary to store relevance judgments\n",
    "    reljudges = {}\n",
    "    # Open the file containing Cranfield relevance judgments\n",
    "    with open('assets/cranfield/cranfield.reljudge', 'r') as file:\n",
    "        for line in file:\n",
    "            # Split each line into query number and document number\n",
    "            q, d = line.split()\n",
    "            # If the query number is not already in the dictionary, add it with an empty list\n",
    "            if q not in reljudges:\n",
    "                reljudges[q] = []\n",
    "            # Append the document number to the list of relevant documents for the query\n",
    "            reljudges[q].append(d)\n",
    "    # Return the relevance judgments dictionary with query IDs and document IDs properly formatted and sorted\n",
    "    return {'q' + k: ['d' + item for item in v] for k, v in sorted(reljudges.items(), key=lambda item: int(item[0]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a555ba8-16ee-4fa1-a1a9-70c3e32fbc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded relevance judgments for 225 queries\n"
     ]
    }
   ],
   "source": [
    "# Load the relevance judgments\n",
    "reljudges = load_cranfield_reljudges()\n",
    "print(f\"Loaded relevance judgments for {len(reljudges)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5a0d5-efaa-4992-bb20-215d67d0ce97",
   "metadata": {},
   "source": [
    "## 3. Building the Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96c07e-ba70-4a54-a901-c5186cda47b6",
   "metadata": {},
   "source": [
    "### Building the Inverted Index\n",
    "\n",
    "An inverted index is a fundamental data structure in information retrieval systems, such as search engines. It allows for efficient full-text searches by mapping terms to the documents that contain them. This means that for each term, the inverted index records which documents contain that term, how frequently it appears, and at which positions within the documents.\n",
    "\n",
    "Our inverted index should look something like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'experiment': {'d1': [1, [0]], ..., 'd30': [2, [12, 40]], ..., 'd123': [3, [11, 45, 67]], ...}, \n",
    "    'studi': {'d1': [1, [1]], 'd2': [2, [0, 36]], ..., 'd207': [3, [19, 44, 59]], ...}\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ac2bf-283e-4ca8-9964-ea69cda5ef6c",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- The key of the outer dictionary is a term (e.g., \"experiment\").\n",
    "- The value is another dictionary where:\n",
    "  - The key is a document ID (e.g., 'd1').\n",
    "  - The value is a list where:\n",
    "    - The first element is the frequency of the term in that document.\n",
    "    - The second element is a list of positions where the term appears in the document.\n",
    "\n",
    "For example, the entry `'d1': [1, [0]]` under the term \"experiment\" means:\n",
    "- Document `d1` contains the term \"experiment\".\n",
    "- The term \"experiment\" appears once in `d1`, at index 0.\n",
    "\n",
    "Similarly, the entry `'d207': [3, [19, 44, 59]]` under the term \"studi\" means:\n",
    "- Document `d207` contains the term \"studi\".\n",
    "- The term \"studi\" appears three times in `d207`, at indices 19, 44, and 59.\n",
    "\n",
    "### Deciding Which Tokens to Include\n",
    "\n",
    "Not all tokens should be tracked in our inverted index. We focus on \"significant\" terms that appear in a reasonable number of documents. Tokens appearing in only one document might be noise, while setting the threshold too high might exclude useful terms. We use the `min_df` parameter to specify the minimum number of documents a token must appear in to be included in the index.\n",
    "\n",
    "### Function to Build the Inverted Index\n",
    "\n",
    "Below is a function to build the inverted index. This function accepts two arguments:\n",
    "\n",
    "- `docs`: A collection of documents as built previously.\n",
    "- `min_df`: The minimum document frequency a token must have to be included in the inverted index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d227c1-f399-4a76-977f-cbe34f7c1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(docs, min_df=1):\n",
    "    \"\"\"\n",
    "    Build an inverted index from the collection of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - docs: A dictionary where keys are doc_ids and values are lists of tokens.\n",
    "    - min_df: Minimum document frequency a token must have to be included in the index.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary representing the inverted index.\n",
    "    \"\"\"\n",
    "    inv_index = {}\n",
    "    \n",
    "    # Iterate over each document and its tokens\n",
    "    for doc_id, tokens in docs.items():\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token not in inv_index:\n",
    "                inv_index[token] = {}\n",
    "            if doc_id not in inv_index[token]:\n",
    "                inv_index[token][doc_id] = [0, []]\n",
    "            inv_index[token][doc_id][0] += 1\n",
    "            inv_index[token][doc_id][1].append(pos)\n",
    "    \n",
    "    # Filter out tokens that do not meet the min_df threshold\n",
    "    inv_index = {term: doc_dict for term, doc_dict in inv_index.items() if len(doc_dict) >= min_df}\n",
    "    \n",
    "    return inv_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61afd1b-7380-430a-b8a6-b51934f2c89c",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "Initialization: The function initializes an empty dictionary inv_index to store the inverted index.\n",
    "\n",
    "Iterating Over Documents: It iterates over each document and its tokens.\n",
    "\n",
    "Building the Index: For each token, the function updates the frequency count and positions within the document in the inv_index.\n",
    "\n",
    "Filtering Terms: After building the initial index, it filters out tokens that do not meet the minimum document frequency (min_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44b7e94-9eae-4ccb-b5cb-e53739bd4987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built inverted index with 1038 terms\n"
     ]
    }
   ],
   "source": [
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(documents, min_df=10)\n",
    "print(f\"Built inverted index with {len(inverted_index)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cedf59-989f-4197-babf-c3ff5fbdf5ca",
   "metadata": {},
   "source": [
    "## 4. Document Retrieval and Ranking\n",
    "\n",
    "Let's implement a function to retrieve and rank documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df01b0-68c8-441c-b6be-8dfa4035f12d",
   "metadata": {},
   "source": [
    "### Retrieving and Ranking Documents\n",
    "\n",
    "Now let's write a function to retrieve and rank documents based on a given query. We'll use a straightforward heuristic:\n",
    "\n",
    "1. For each term in the query, accumulate the term frequencies for each document that contains the term.\n",
    "2. Repeat this process for all terms in the query.\n",
    "3. Rank the documents in descending order of their total term frequencies.\n",
    "4. If two documents have the same total term frequencies, the document with the lower document number should be ranked higher. For example, if `d12` and `d100` have the same total term frequencies, `d12` should be ranked higher than `d100` because 12 < 100.\n",
    "\n",
    "This approach ensures that the most relevant documents appear first in the search results.\n",
    "\n",
    "### Function Details\n",
    "\n",
    "The function takes the following inputs:\n",
    "- `inverted_index`: A dictionary returned by the inverted index function, mapping terms to their occurrences in documents.\n",
    "- `queries`: A dictionary of queries, where each key is a query ID and each value is a list of terms.\n",
    "- `max_docs`: An optional parameter to limit the number of documents retrieved to `max_docs`. If `max_docs` is set to -1, it retrieves all matched documents without limit.\n",
    "\n",
    "The function outputs a dictionary containing an ordered list of retrieved documents for each query, formatted like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1'  : ['d51', 'd874', ..., 'd717'], \n",
    "    'q2'  : ['d51', 'd1147', ..., 'd14'],\n",
    "    ...,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df836e99-ad43-4103-8a57-f361fd4843f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_n_rank_docs(inverted_index, queries, max_docs=-1):\n",
    "    \"\"\"\n",
    "    Retrieve and rank documents based on the queries using an inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    - inverted_index: A dictionary where keys are terms and values are dictionaries\n",
    "      mapping document IDs to term frequency and positions.\n",
    "    - queries: A dictionary of queries where keys are query IDs and values are lists of terms.\n",
    "    - max_docs: Maximum number of documents to retrieve for each query. If -1, retrieve all matched documents.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where each key is a query ID and each value is a list of document IDs in order of relevance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for q_id, terms in queries.items():\n",
    "        doc_scores = {}\n",
    "        \n",
    "        for term in terms:\n",
    "            if term in inverted_index:\n",
    "                for doc_id, (freq, _) in inverted_index[term].items():\n",
    "                    if doc_id not in doc_scores:\n",
    "                        doc_scores[doc_id] = 0\n",
    "                    doc_scores[doc_id] += freq\n",
    "        \n",
    "        # Sort documents first by score (descending) then by doc_id (ascending)\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: (-x[1], int(x[0][1:])))\n",
    "        \n",
    "        # Limit the number of documents if max_docs is specified\n",
    "        if max_docs == -1:\n",
    "            results[q_id] = [doc_id for doc_id, _ in sorted_docs]\n",
    "        else:\n",
    "            results[q_id] = [doc_id for doc_id, _ in sorted_docs[:max_docs]]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341176b-5066-49f9-8e28-84826af1e065",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "**Initialization**: The function initializes an empty dictionary `results` to store the final ranked documents for each query.\n",
    "\n",
    "**Term Matching**: For each term in the query, the function checks if the term exists in the inverted index. If it does, it accumulates the term frequencies for each document.\n",
    "\n",
    "**Ranking**: After accumulating term frequencies, the function sorts the documents first by their term frequency in descending order, and then by their document ID in ascending order if there's a tie.\n",
    "\n",
    "**Document Limiting**: If `max_docs` is specified, it limits the number of documents returned for each query. Otherwise, it returns all matched documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9697850-9e52-46d8-9ad4-eb138e0f0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents for 225 queries\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and rank documents\n",
    "retrieved_docs = retrieve_n_rank_docs(inverted_index, queries, max_docs=10)\n",
    "print(f\"Retrieved documents for {len(retrieved_docs)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2174e2-b7d7-4a9f-b83a-c2b8e4067647",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "### 5.1 Precision and Recall\n",
    "\n",
    "Let's calculate precision and recall:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016ccfc-d5db-4743-a73a-7ea6111ad879",
   "metadata": {},
   "source": [
    "### Calculating Precision and Recall at n\n",
    "\n",
    "We need to write a function to calculate precision and recall at a specific cutoff `n`. This function takes the following inputs:\n",
    "\n",
    "- `ret_docs`: A dictionary of retrieved documents as returned by the document retrieval function.\n",
    "- `reljudges`: A dictionary of relevance judgments as returned by the relevance judgments function.\n",
    "- `n`: The cutoff value to limit the number of documents considered in the calculation.\n",
    "\n",
    "#### Precision at n\n",
    "\n",
    "Precision at `n` measures the proportion of retrieved documents that are relevant within the top `n` results. It provides an indication of the accuracy of the retrieval system in returning relevant documents at the top of the search results.\n",
    "\n",
    "A special case arises when `n=-1` or when `n` exceeds the number of documents retrieved. In these cases, we calculate precision and recall based on all retrieved documents. For example, if 10 documents were retrieved for a query, then precision at 12 is the same as precision at 10 (and precision at -1 by our rules).\n",
    "\n",
    "The function should output two dictionaries containing the precision and recall at `n` for each query in `ret_docs`, respectively, formatted like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1'  : 0.1,\n",
    "    'q2'  : 0.3,\n",
    "    ...,\n",
    "    'q225': 0.2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aefde8b3-26cb-4df6-a3b8-fb81c6cca83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pre_rec_at_n(ret_docs, reljudges, n=-1):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall at n for each query in ret_docs.\n",
    "\n",
    "    Parameters:\n",
    "    - ret_docs: A dictionary where keys are query IDs and values are lists of retrieved document IDs.\n",
    "    - reljudges: A dictionary where keys are query IDs and values are lists of relevant document IDs.\n",
    "    - n: The cutoff value for the number of documents to consider.\n",
    "\n",
    "    Returns:\n",
    "    - Two dictionaries: one for precision at n and one for recall at n.\n",
    "    \"\"\"\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    \n",
    "    for q_id in ret_docs:\n",
    "        retrieved_docs = ret_docs[q_id]\n",
    "        relevant_docs = reljudges.get(q_id, [])\n",
    "        \n",
    "        # Determine the effective cutoff\n",
    "        if n == -1 or n > len(retrieved_docs):\n",
    "            effective_n = len(retrieved_docs)\n",
    "        else:\n",
    "            effective_n = n\n",
    "        \n",
    "        # Calculate true positives\n",
    "        true_positives = len(set(retrieved_docs[:effective_n]) & set(relevant_docs))\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision[q_id] = true_positives / effective_n if effective_n > 0 else 0\n",
    "        recall[q_id] = true_positives / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a55508-b79e-4944-bd1a-88fb188c843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision: 0.0413\n",
      "Average recall: 0.0619\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall\n",
    "precision, recall = calc_pre_rec_at_n(retrieved_docs, reljudges)\n",
    "print(f\"Average precision: {sum(precision.values()) / len(precision):.4f}\")\n",
    "print(f\"Average recall: {sum(recall.values()) / len(recall):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26072c94-43e5-4afc-9c0f-032a011a8c18",
   "metadata": {},
   "source": [
    "### 5.2 Average Precision\n",
    "\n",
    "Now, let's calculate average precision:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c4cb6-c05b-417d-8037-d0fdf93ed899",
   "metadata": {},
   "source": [
    "### Calculating Average Precision and Mean Average Precision\n",
    "\n",
    "Another commonly used metric in information retrieval is average precision (AP). This metric helps evaluate the precision of the retrieval system at different cutoff levels and gives a single-figure measure of quality across recall levels. \n",
    "\n",
    "#### Average Precision (AP)\n",
    "\n",
    "Average Precision calculates the precision at each relevant document retrieved, and then averages these precisions over the total number of relevant documents for a query. It provides a single-figure measure of quality across recall levels by considering the rank of each retrieved relevant document.\n",
    "\n",
    "#### Mean Average Precision (MAP)\n",
    "\n",
    "Mean Average Precision is the mean of the average precision scores for all queries. It provides an overall evaluation metric for the system across multiple queries.\n",
    "\n",
    "### The Function\n",
    "\n",
    "We will write a function to calculate the average precision for each query and the mean average precision for all queries. This function accepts the same arguments as the `calc_pre_rec_at_n` function:\n",
    "\n",
    "- `ret_docs`: A dictionary of retrieved documents.\n",
    "- `reljudges`: A dictionary of relevance judgments.\n",
    "- `cutoff`: The cutoff value to limit the number of documents considered as \"retrieved.\"\n",
    "\n",
    "The `cutoff` parameter serves a similar purpose to `n` in the previous function: only documents ranked higher than or at the cutoff are considered. If `cutoff` is -1 or exceeds the number of documents retrieved, all retrieved documents are considered.\n",
    "\n",
    "The function should output a dictionary representing the average precision for each query and a float representing the mean average precision. The dictionary should look similar to:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1'  : 0.03571428571428571,\n",
    "    'q2'  : 0.08194444444444444,\n",
    "    ...,\n",
    "    'q225': 0.02023809523809524\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15c07c01-56ae-4b57-a0a1-fba8fdb30573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_pre(ret_docs, reljudges, cutoff=-1):\n",
    "    # Initialize a dictionary to store average precision for each query\n",
    "    avg_pre = {}\n",
    "    \n",
    "    for query in ret_docs.keys():\n",
    "        # Determine the effective cutoff\n",
    "        effective_cutoff = min(cutoff, len(ret_docs[query])) if cutoff != -1 else len(ret_docs[query])\n",
    "        \n",
    "        # Initialize a list to store precision values and a counter for relevant documents\n",
    "        precision = []\n",
    "        counter = 0\n",
    "        \n",
    "        # Iterate through the retrieved documents up to the effective cutoff\n",
    "        for i, doc in enumerate(ret_docs[query][:effective_cutoff], 1):\n",
    "            if doc in reljudges[query]:\n",
    "                counter += 1\n",
    "                precision.append(counter / i)\n",
    "        \n",
    "        # Calculate average precision for the query\n",
    "        avg_pre[query] = sum(precision) / len(reljudges[query]) if reljudges[query] else 0\n",
    "    \n",
    "    # Calculate mean average precision across all queries\n",
    "    mean_avg_pre = sum(avg_pre.values()) / len(avg_pre)\n",
    "    \n",
    "    return avg_pre, mean_avg_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3798b20a-9412-46c2-a4ca-bb92cd3bfa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision: 0.0190\n"
     ]
    }
   ],
   "source": [
    "# Calculate average precision\n",
    "avg_precision, mean_avg_precision = calc_avg_pre(retrieved_docs, reljudges)\n",
    "print(f\"Mean Average Precision: {mean_avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bcbf0-0860-4c49-8fea-1a9658731010",
   "metadata": {},
   "source": [
    "### 5.3 Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "Finally, let's calculate NDCG:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7b3ad-1970-48a5-91bf-e143b6d2c004",
   "metadata": {},
   "source": [
    "### Evaluating with Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "Finally, let's evaluate our text retrieval system using Normalized Discounted Cumulative Gain (NDCG). NDCG is a popular metric in information retrieval that accounts for both the relevance and the ranking of the retrieved documents. \n",
    "\n",
    "#### Understanding NDCG\n",
    "\n",
    "NDCG measures the usefulness (gain) of a document based on its position in the result list. The gain is discounted logarithmically proportional to the position of the result, which means that higher-ranked documents contribute more to the overall gain. NDCG is then normalized over the ideal gain, providing a score between 0 and 1.\n",
    "\n",
    "##### Calculating NDCG\n",
    "\n",
    "To calculate NDCG, each document in the result set must have a numerical relevance score. However, the Cranfield collection provides only binary relevance judgments (relevant or not relevant). To work around this, we make the following assumptions:\n",
    "\n",
    "- Irrelevant documents (those not in `reljudges`) have a relevance score of 1.\n",
    "- Relevant documents are assigned decreasing relevance scores in the order they appear in `reljudges`.\n",
    "\n",
    "For example, consider the following relevance judgments:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1': ['d184', 'd29', 'd879', 'd880'],\n",
    "    'q2': ['d12', 'd15', 'd658']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333d8b6-4d47-4393-93c8-ea4fc7d09840",
   "metadata": {},
   "source": [
    "We assign the following relevance scores to the relevant documents of each query:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1': [5, 4, 3, 2],\n",
    "    'q2': [4, 3, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d91dc8-93e3-40d7-88f4-777500cec4fb",
   "metadata": {},
   "source": [
    "For q1, the document d184 is assigned a relevance score of 5, d29 a score of 4, and so on.\n",
    "For q2, the document d12 is assigned a relevance score of 4, d15 a score of 3, and d658 a score of 2.\n",
    "The last relevant document always gets a relevance score of 2, the second last gets 3, and so on, until the first relevant document. The relevance scores increase by 1 each time we move away from the last relevant document, starting at 2.\n",
    "\n",
    "The Function\n",
    "The function to calculate NDCG under these assumptions accepts the following arguments:\n",
    "\n",
    "ret_docs: A dictionary of retrieved documents.\n",
    "reljudges: A dictionary of relevance judgments.\n",
    "n: The cutoff value for the number of documents to consider.\n",
    "base: The base of the logarithm function used to discount relevance scores.\n",
    "When n=-1 or when n exceeds the number of documents retrieved, we calculate NDCG based on all retrieved documents. The function outputs a dictionary containing the NDCG of each query, formatted like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'q1': 0.21628669853396418,\n",
    "    'q2': 0.3984097639816684,\n",
    "    ...,\n",
    "    'q225': 0.1361909750034715\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e13420-40fb-4279-bc13-e6f173b05327",
   "metadata": {},
   "source": [
    "Each key in the dictionary is a query ID (q_id), and each value is a float representing the NDCG for that query.\n",
    "\n",
    "Corner Cases\n",
    "An important corner case arises when n exceeds the number of relevant documents. In this case, the top few documents in the ideal ranking are the relevant documents in descending order of their relevance scores. After the list of relevant documents is exhausted, we treat the remaining documents as irrelevant with a relevance score of 1.\n",
    "\n",
    "This function returns a dictionary of length equal to the number of queries in ret_docs, where each key is a query ID (q_id) and each value is a float representing the NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8382dd-4640-4611-b38f-6981d8eb2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_NDCG_at_n(ret_docs, reljudges, n=-1, base=2):\n",
    "    # Initialize a dictionary to store NDCG for each query\n",
    "    ndcg = {}\n",
    "    \n",
    "    for query in ret_docs.keys():\n",
    "        # Determine the actual number of documents to consider\n",
    "        actual_n = min(n, len(ret_docs[query])) if n != -1 else len(ret_docs[query])\n",
    "        \n",
    "        # Retrieve the top documents up to the actual_n\n",
    "        ret = ret_docs[query][:actual_n]\n",
    "        \n",
    "        # Get the list of relevant documents for the query\n",
    "        judge = reljudges[query]\n",
    "        \n",
    "        # Assign scores to the relevant documents\n",
    "        judge_scores = {doc_id: len(judge) + 1 - i for i, doc_id in enumerate(judge)}\n",
    "        \n",
    "        # Calculate the DCG for the retrieved documents\n",
    "        dcg = sum(judge_scores.get(doc_id, 1) / (math.log(i + 2, base) if i >= base else 1) for i, doc_id in enumerate(ret))\n",
    "        \n",
    "        # Calculate the ideal DCG\n",
    "        ideal_dcg = sum(score / (math.log(i + 2, base) if i >= base else 1) for i, score in enumerate(sorted(judge_scores.values(), reverse=True)[:actual_n]))\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        ndcg[query] = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "    \n",
    "    return ndcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f538e6c-aeed-4ade-8222-b83d3a268225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG: 0.4475\n"
     ]
    }
   ],
   "source": [
    "# Calculate NDCG\n",
    "ndcg = calc_NDCG_at_n(retrieved_docs, reljudges)\n",
    "print(f\"Average NDCG: {sum(ndcg.values()) / len(ndcg):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdec24f-40f9-4bfb-aeec-315e80d68d1f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a complete mini search engine, from data loading and preprocessing to evaluation. We've implemented key information retrieval concepts such as inverted indexing, document ranking, and various evaluation metrics. This forms a solid foundation for understanding more complex search systems and information retrieval techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
